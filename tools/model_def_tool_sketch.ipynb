{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_jl_files (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Functions to find all the Julia files in a directory and subdirectories\n",
    "function filter_jl_files(dir_list::Vector{String})\n",
    "    \"\"\"Filter a list of filenames into a list of Julia files and a list of non-Julia files\"\"\"\n",
    "    jl_files = filter(x -> endswith(x, \".jl\"), dir_list)\n",
    "    non_jl_files = setdiff(dir_list, jl_files)\n",
    "    return jl_files, non_jl_files\n",
    "end\n",
    "\n",
    "function add_filepaths!(filepaths::Dict{String, String}, dir_path::String, dir_list::Vector{String})\n",
    "    \"\"\"Add a list of filepaths to a dictionary\"\"\"\n",
    "    for file in dir_list\n",
    "        filepaths[file] = joinpath(dir_path, file)\n",
    "    end\n",
    "end\n",
    "\n",
    "function find_jl_files(dir_path::String)\n",
    "    \"\"\"Find all Julia files in a directory and subdirectories\"\"\"\n",
    "    filepaths = Dict{String, String}()\n",
    "    jl_files, non_jl_files = filter_jl_files(readdir(dir_path))\n",
    "    add_filepaths!(filepaths, dir_path, jl_files)\n",
    "    new_dirs = filter(x -> isdir(joinpath(dir_path, x)), non_jl_files)\n",
    "    for new_dir in new_dirs\n",
    "        merge!(filepaths, find_jl_files(joinpath(dir_path, new_dir)))\n",
    "    end\n",
    "    return filepaths\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_filestrings (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Functions to read a Julia file, clean it, and return it as a long string\n",
    "function clean_file(dir_path::String)\n",
    "    \"\"\"\n",
    "        filestring = clean_file(dir_path::String)\n",
    "\n",
    "    Read a Julia file line by line,\n",
    "    removing all comments and doc strings,\n",
    "    then appending each line to a string with a space\n",
    "    \"\"\"\n",
    "    open(dir_path, \"r\") do file\n",
    "        filestring = \"\"\n",
    "        long_comment = false\n",
    "        for line in eachline(file)\n",
    "            # Check for doc strings and ignore them\n",
    "            if contains(line, \"\"\"\\\"\\\"\\\"\"\"\")\n",
    "                long_comment = !long_comment\n",
    "            end\n",
    "            if long_comment\n",
    "                continue\n",
    "            else \n",
    "                # Remove comments\n",
    "                if !startswith(strip(line), \"#\") && line != \"\"\n",
    "                    # Remove end-of-line commments\n",
    "                    line = split(line, \"#\")[1]\n",
    "                    # Remove tabs\n",
    "                    line = replace(line, \"\\t\" => \"  \")\n",
    "                    # Make all one line, but add space to avoid concatenating words\n",
    "                    filestring = string(filestring, \" \", line)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        return filestring\n",
    "    end\n",
    "end\n",
    "\n",
    "function make_filestrings(filepath::Dict{String, String})\n",
    "    \"\"\"Make a dictionary of filestrings from a dictionary of filepaths\"\"\"\n",
    "    filestrings = Dict{String, String}()\n",
    "    for (filename, filepath) in filepaths\n",
    "        filestrings[filename] = clean_file(filepath)\n",
    "    end\n",
    "    return filestrings\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_feature_creation! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Functions to find the names and creation-file of model features from a filestring\n",
    "function extract_feature_name(filestring::AbstractString, filter_prefix::AbstractString)\n",
    "    \"\"\"Extract the name of a model feature from a file-substring\"\"\"\n",
    "    x = split(filestring, \",\")\n",
    "    # Try to extract the name based on a prefix\n",
    "    x2 = split(x[2], \" \")\n",
    "    x2 = filter(x -> startswith(x, filter_prefix), x2)\n",
    "    if !isempty(x2)\n",
    "        x = x2[1]\n",
    "    else\n",
    "        x = x[2]\n",
    "    end\n",
    "    # Remove any indexing\n",
    "    if contains(x, \"[\")\n",
    "        x = split(x, \"[\")[1]\n",
    "    end\n",
    "    # Remove whitespace\n",
    "    return strip(x)\n",
    "end\n",
    "\n",
    "function find_feature_names(filestring::String, filter_string::String)\n",
    "    \"\"\"Create a list of the names of all the model feature of a given type, from a filestring\"\"\"\n",
    "    cases = split(filestring, filter_string)\n",
    "    feature_names = Vector{String}()\n",
    "    filter_prefix = string(filter_string[2])\n",
    "    for i in 2:length(cases)\n",
    "        push!(feature_names, extract_feature_name(cases[i], filter_prefix))\n",
    "    end\n",
    "    return feature_names\n",
    "end\n",
    "\n",
    "function find_feature_creation!(feature_results::Dict{String, Any}, filestrings::Dict{String, String}, feature_name::String)\n",
    "    \"\"\"Find the files in which a set of features is created\"\"\"\n",
    "    for (filename, filestring) in filestrings\n",
    "        new_feature_names = find_feature_names(filestring, feature_name)\n",
    "        for new_name in new_feature_names\n",
    "            if !haskey(feature_results, new_name)\n",
    "                feature_results[new_name] = Dict{String, Any}()\n",
    "            end\n",
    "            feature_results[new_name][\"created\"] = filename\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_feature_accesses! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Functions to find where each model feature is accessed\n",
    "function check_access(filestrings::Dict{String, String}, feature_names::Vector{String})\n",
    "    \"\"\"Check if features are accessed in a set of files, using filestrings\"\"\"\n",
    "    file_accesses = Dict{String, Vector{String}}()\n",
    "    for (filename, filestring) in filestrings\n",
    "        file_accesses[filename] = Vector{String}()\n",
    "        for feature_name in feature_names\n",
    "            if contains(filestring, feature_name)\n",
    "                push!(file_accesses[filename], feature_name)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return file_accesses\n",
    "end\n",
    "\n",
    "function push_accesses!(feature_results::Dict{String, Any}, accesses::Dict{String, Vector{String}})\n",
    "    \"\"\"Push a list of features accesses to a feature result dictionary\"\"\"\n",
    "    for (filename, feature_names) in accesses\n",
    "        for feature_name in feature_names\n",
    "            if !haskey(feature_results[feature_name], \"accessed\")\n",
    "                feature_results[feature_name][\"accessed\"] = Vector{String}()\n",
    "            end\n",
    "            push!(feature_results[feature_name][\"accessed\"], filename)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function find_feature_accesses!(feature_results::Dict{String, Any}, filestrings::Dict{String, String})\n",
    "    \"\"\"Find the files in which a set of features is accessed\"\"\"\n",
    "    accesses = check_access(filestrings, collect(keys(feature_results)))\n",
    "    push_accesses!(feature_results, accesses)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_2_file (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Functions to turn search_results into a Markdown table\n",
    "function make_table_header(headers::Set{String}, feature_type::String)\n",
    "    fun_headers = copy(headers)\n",
    "    table_header = \"|$(feature_type) name\"\n",
    "    table_divider = \"|:-\"\n",
    "    if \"created\" in fun_headers # Later we can automate this using the features_to_search superdetails\n",
    "        table_header = string(table_header, \"|created\")\n",
    "        table_divider = string(table_divider, \"|:-\")\n",
    "        delete!(fun_headers, \"created\")\n",
    "    end    \n",
    "    for header in fun_headers\n",
    "        table_header = string(table_header, \"|\", header)\n",
    "        table_divider = string(table_divider, \"|\", \":-\")\n",
    "    end\n",
    "    table_header = string(table_header, \"|\")\n",
    "    table_divider = string(table_divider, \"|\")\n",
    "    return table_header, table_divider\n",
    "end\n",
    "\n",
    "function make_table_row(feature_name::String, feature_results::Dict{String, Any}, headers::Set{String})\n",
    "    table_row = \"|$(feature_name)\"\n",
    "    fun_headers = copy(headers)\n",
    "    if \"created\" in fun_headers\n",
    "        table_row = string(table_row, \"|\", feature_results[\"created\"])\n",
    "        delete!(fun_headers, \"created\")\n",
    "    end\n",
    "    for header in fun_headers\n",
    "        table_row = string(table_row, \"|\", join(feature_results[header], \", \"))\n",
    "    end\n",
    "    table_row = string(table_row, \"|\")\n",
    "    return table_row\n",
    "end\n",
    "\n",
    "function features_2_markdown(feature_results::Dict{String, Any}, feature_type::String, headers::Set{String})\n",
    "    md_table = Vector{String}()\n",
    "    push!(md_table, \"## $(feature_type)s\")\n",
    "    table_header, table_divider = make_table_header(headers, feature_type)\n",
    "    push!(md_table, table_header)\n",
    "    push!(md_table, table_divider)\n",
    "    sorted_names = sort(collect(keys(feature_results)))\n",
    "    for name in sorted_names\n",
    "        push!(md_table, make_table_row(name, feature_results[name], headers))\n",
    "    end\n",
    "    return md_table\n",
    "end\n",
    "\n",
    "function write_2_file(filename::String, md_table::Vector{String})\n",
    "    open(filename, \"w\") do f\n",
    "        for line in md_table\n",
    "            write(f, string(line, \"\\n\"))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function write_2_file(filename::String, md_table::Vector{String}, open_mode::String)\n",
    "    open(filename, open_mode) do f\n",
    "        for line in md_table\n",
    "            write(f, string(line, \"\\n\"))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds all the variables and expressions, and prints them as a markdown table\n",
    "\n",
    "# Get all Julia files in the src directory\n",
    "src_path = joinpath(dirname(pwd()), \"src\")\n",
    "filepaths = find_jl_files(src_path)\n",
    "\n",
    "outpath = joinpath(pwd(), \"model_table.md\")\n",
    "open(outpath, \"w\") do io\n",
    "    println(io, \"# Model Variables and Expressions\")\n",
    "end\n",
    "\n",
    "# Clean all Julia files and return them as strings\n",
    "filestrings = make_filestrings(filepaths)\n",
    "\n",
    "# Create a dictionary of all the model features we'd like to search for, and the details of interest\n",
    "features_to_search = Dict(\n",
    "    \"var\" => Dict(\"filter_string\" => \"@variable\", \"table_name\" => \"Variable\", \"details\" => [\"created\", \"accessed\"], \"superdetails\" => [\"created\"]),\n",
    "    \"exp\" => Dict(\"filter_string\" => \"@expression\", \"table_name\" => \"Expression\", \"details\" => [\"created\", \"accessed\"], \"superdetails\" => [\"created\"])\n",
    ")\n",
    "\n",
    "# Create dictionary to hold search results\n",
    "# We could be more specific and make the last Dict{String, String||List}, but this is safer\n",
    "search_results = Dict{String, Dict{String, Any}}()\n",
    "for (search_name, search_details) in features_to_search\n",
    "    search_results[search_name] = Dict{String, Dict{String, Any}}()\n",
    "end\n",
    "\n",
    "# Our dict of results is not ordered, so we specify that here\n",
    "print_order = Vector{String}([\"var\", \"exp\"])\n",
    "\n",
    "# Find the file in which each feature is created\n",
    "for search_name in print_order\n",
    "    search_details = features_to_search[search_name]\n",
    "    find_feature_creation!(search_results[search_name], filestrings, search_details[\"filter_string\"])\n",
    "    find_feature_accesses!(search_results[search_name], filestrings)\n",
    "    md_table = features_2_markdown(search_results[search_name], search_details[\"table_name\"], Set(search_details[\"details\"]))\n",
    "    write_2_file(outpath, md_table, \"a\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{String, Vector{String}}} with 145 entries:\n",
       "  \"h2_production_commit.jl\"          => Dict(\"h2_production_commit\"=>[\"EP::Mode…\n",
       "  \"write_reserve_margin_revenue.jl\"  => Dict(\"write_reserve_margin_revenue\"=>[\"…\n",
       "  \"load_co2_cap.jl\"                  => Dict(\"load_co2_cap\"=>[\"setup::Dict\", \"p…\n",
       "  \"flexible_demand.jl\"               => Dict(\"flexible_demand\"=>[\"EP::Model\", \"…\n",
       "  \"investment_discharge.jl\"          => Dict(\"investment_discharge\"=>[\"EP::Mode…\n",
       "  \"write_reg.jl\"                     => Dict(\"write_reg\"=>[\"path::AbstractStrin…\n",
       "  \"write_time_weights.jl\"            => Dict(\"write_time_weights\"=>[\"path::Abst…\n",
       "  \"load_inputs.jl\"                   => Dict(\"load_inputs\"=>[\"setup::Dict,path:…\n",
       "  \"emissions_power.jl\"               => Dict(\"emissions_power\"=>[\"EP::Model\", \"…\n",
       "  \"load_period_map.jl\"               => Dict(\"load_period_map\"=>[\"setup::Dict,p…\n",
       "  \"h2_pipeline.jl\"                   => Dict(\"h2_pipeline\"=>[\"EP::Model\", \"inpu…\n",
       "  \"load_h2_g2p_variability.jl\"       => Dict(\"load_h2_g2p_variability\"=>[\"setup…\n",
       "  \"write_curtailment.jl\"             => Dict(\"write_curtailment\"=>[\"path::Abstr…\n",
       "  \"write_h2_pipeline_expansion.jl\"   => Dict(\"write_h2_pipeline_expansion\"=>[\"p…\n",
       "  \"h2_production_all.jl\"             => Dict(\"h2_production_all\"=>[\"EP::Model\",…\n",
       "  \"load_energy_share_requirement.jl\" => Dict(\"load_energy_share_requirement\"=>[…\n",
       "  \"write_charge.jl\"                  => Dict(\"write_charge\"=>[\"path::AbstractSt…\n",
       "  \"write_h2_truck_flow.jl\"           => Dict(\"write_h2_truck_flow\"=>[\"path::Abs…\n",
       "  \"discharge.jl\"                     => Dict(\"discharge\"=>[\"EP::Model\", \"inputs…\n",
       "  ⋮                                  => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Functions to find all functions in a file and their arguments\n",
    "function extract_function_names_and_args(filestring::String)\n",
    "    \"\"\"Extract function names and arguments from a filestring\"\"\"\n",
    "    function_args = Dict{String, Vector{String}}()\n",
    "    filesubstrings = split(filestring, \"function \")\n",
    "    filesubstrings = filesubstrings[2:end]\n",
    "    for substring in filesubstrings\n",
    "        substring = split(substring, \"(\")\n",
    "        function_name = substring[1]\n",
    "        function_arg = split(split(substring[2], \")\")[1], \", \")\n",
    "        function_args[function_name] = function_arg\n",
    "    end\n",
    "    return function_args\n",
    "end\n",
    "\n",
    "function find_functions(filestrings::Dict{String, String})\n",
    "    \"\"\"Find all functions in a set of filestrings\"\"\"\n",
    "    functions_dict = Dict{String, Dict{String, Vector{String}}}()\n",
    "    for (filename, filestring) in filestrings\n",
    "        functions_dict[filename] = extract_function_names_and_args(filestring)\n",
    "    end\n",
    "    return functions_dict\n",
    "end\n",
    "\n",
    "functions_dict = find_functions(filestrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exp_2_markdown (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function find_expression_mutations(filestring::String, expression_name::String, model_name::String)\n",
    "    \"\"\"\n",
    "    Find all mutations of an expression in a file, based on a filestring\\n\n",
    "    TODO: This is a really weak approach and would be better to handle with the parser.\n",
    "    \"\"\"\n",
    "    expression_mutations = Vector{String}()\n",
    "    # Assume that these expression mutations are not indexed, so we add a space after the name\n",
    "    exp_filter_string = string(model_name, \"[:\", expression_name, \"] \")\n",
    "    filesubstrings = split(filestring, exp_filter_string)[2:end]\n",
    "    for substring in filesubstrings\n",
    "        # For now assume that all mutations are single lines -- TODO: This is really weak\n",
    "        mut = split(substring, \"  \")[1]\n",
    "        # Simple check that what we've found is a mutation\n",
    "        try Meta.parse(string(exp_filter_string, \" \", mut))\n",
    "            push!(expression_mutations, mut)\n",
    "        catch\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    return expression_mutations\n",
    "end\n",
    "\n",
    "function exp_2_markdown(exp_mutations::Dict{String, Vector{String}}, exp_name::String)\n",
    "    md_table = Vector{String}()\n",
    "    push!(md_table, \"## $(exp_name)\")\n",
    "    table_header = \"|Mutation|File|Explanation| \"\n",
    "    table_divider = \"|:-|:-|:-\"\n",
    "    push!(md_table, table_header)\n",
    "    push!(md_table, table_divider)\n",
    "    sorted_names = sort(collect(keys(exp_mutations)))\n",
    "    for name in sorted_names\n",
    "        for mut in exp_mutations[name]\n",
    "            push!(md_table, string(\"|\", mut, \"|\", name, \"|\", \" \", \"|\"))\n",
    "        end\n",
    "    end\n",
    "    return md_table\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incomplete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"incomplete: premature end of input\"]\n"
     ]
    }
   ],
   "source": [
    "# Options looking at the Julia parser\n",
    "s = \"string(\\\"Ruaridh\\\", \\\" Jamie\\\")\"\n",
    "s = \"1 +\"\n",
    "w = Meta.parse(s)\n",
    "typeof(w)\n",
    "println(w.head)\n",
    "println(w.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{String, Vector{String}}} with 3 entries:\n",
       "  \"eObj\"          => Dict(\"h2_production_commit.jl\"=>[\"+= eTotalH2GenCStart\"], …\n",
       "  \"ePowerBalance\" => Dict(\"h2_production_commit.jl\"=>[\"+= -ePowerBalanceH2GenCo…\n",
       "  \"eH2Balance\"    => Dict(\"h2_pipeline.jl\"=>[\"+= ePipeZoneDemand\"], \"h2_product…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outpath = joinpath(pwd(), \"expression_mutations_table.md\")\n",
    "open(outpath, \"w\") do io\n",
    "    println(io, \"# Expression Mutations\")\n",
    "end\n",
    "\n",
    "exp_names = [\n",
    "    \"eObj\", \n",
    "    \"ePowerBalance\", \n",
    "    \"eH2Balance\", \n",
    "    ] \n",
    "\n",
    "expression_mutations = Dict{String, Dict{String, Vector{String}}}()\n",
    "for exp_name in exp_names\n",
    "    expression_mutations[exp_name] = Dict{String, Vector{String}}(exp_name => Vector{String}())\n",
    "    for (filename, filestring) in filestrings\n",
    "        mutations = find_expression_mutations(filestring, exp_name, \"EP\")\n",
    "        if length(mutations) > 0\n",
    "            expression_mutations[exp_name][filename] = mutations\n",
    "        end\n",
    "    end\n",
    "    md_table = exp_2_markdown(expression_mutations[exp_name], exp_name)\n",
    "    write_2_file(outpath, md_table, \"a\")\n",
    "end\n",
    "\n",
    "expression_mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Any}:\n",
       " :([y in THERM_NO_COMMIT, t in START_SUBPERIODS])\n",
       " :((EP[:vP])[y, t] - (EP[:vP])[y, (t + hours_per_subperiod) - 1] <= (dfGen[!, :Ramp_Up_Percentage])[y] * (EP[:eTotalCap])[y])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Method to find @constraints in a filestring\n",
    "# This is much more challenging as they're not named\n",
    "# And many of them are only made under certain conditions\n",
    "# And can be mutually exclusive.\n",
    "# Naming the constraints may avoid accidental conflicting constraints\n",
    "filestring = filestrings[\"thermal_no_commit.jl\"]\n",
    "filesubstrings = split(filestring, \"@constraints(\")\n",
    "substring = filesubstrings[2]\n",
    "substring = split(substring, \"begin\")[2]\n",
    "substring = split(substring, \"end\")[1]\n",
    "substring = split(substring, \"   \")[2]\n",
    "Meta.parse(substring).args\n",
    "# for s in substring\n",
    "    # delete!()\n",
    "# end\n",
    "# substring = split(substring, r\"[<=>]=\")\n",
    "# substring = split(\"substring\", [\"==\", \">=\", \"<=\"])\n",
    "# Meta.parse(substring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: dir_path not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: dir_path not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ d:\\github_copies\\DOLPHYN-dev\\tools\\model_def_tool_sketch.ipynb:1"
     ]
    }
   ],
   "source": [
    "open(dir_path, \"r\") do file\n",
    "    filestring = \"\"\n",
    "    long_comment = false\n",
    "    for line in eachline(file)\n",
    "        # Check for doc strings and ignore them\n",
    "        if contains(line, \"\"\"\\\"\\\"\\\"\"\"\")\n",
    "            long_comment = !long_comment\n",
    "        end\n",
    "        if long_comment\n",
    "            continue\n",
    "        else \n",
    "            # Remove comments\n",
    "            if !startswith(strip(line), \"#\") && line != \"\"\n",
    "                # Remove end-of-line commments\n",
    "                line = split(line, \"#\")[1]\n",
    "                # Remove tabs\n",
    "                line = replace(line, \"\\t\" => \"  \")\n",
    "                # Make all one line, but add space to avoid concatenating words\n",
    "                filestring = string(filestring, \" \", line)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return filestring\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector{SubString{String}}[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Any[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function minify_file(filepath::String)\n",
    "    \"\"\"Minify a file\"\"\"\n",
    "    minified_lines = Vector{String}()\n",
    "    open(filepath, \"r\") do io\n",
    "        long_comment = false\n",
    "        for line in eachline(io)\n",
    "            # Check for doc strings and ignore them\n",
    "            if contains(line, \"\"\"\\\"\\\"\\\"\"\"\")\n",
    "                long_comment = !long_comment\n",
    "                continue\n",
    "            end\n",
    "            if !long_comment && !startswith(strip(line), \"#\") && line != \"\"\n",
    "                    # Remove end-of-line commments\n",
    "                    line = split(line, \"#\")[1]\n",
    "                    # Remove tabs\n",
    "                    line = replace(line, \"\\t\" => \"\")\n",
    "                    # Push to output vector\n",
    "                    push!(minified_lines, line)\n",
    "            end\n",
    "        end\n",
    "        return minified_lines\n",
    "    end\n",
    "end\n",
    "\n",
    "function parse_lines(lines::Vector{String})\n",
    "    \"\"\"Parse a vector of lines\"\"\"\n",
    "    output_exprs = Vector{Any}()\n",
    "    active_str = \"\"\n",
    "    for line in lines\n",
    "        active_str = string(active_str, \"\\n\", line)\n",
    "        try Meta.parse(active_str)\n",
    "            parse = Meta.parse(active_str)\n",
    "            if parse.head == :incomplete\n",
    "                continue\n",
    "            end\n",
    "            reduced_lines = [split(active_str, \"\\n\")][2:end-1]\n",
    "            println(reduced_lines)\n",
    "            push!(output_exprs, parse_lines(reduced_lines))\n",
    "            active_str = \"\"\n",
    "        catch\n",
    "            continue\n",
    "        end\n",
    "    end\n",
    "    return output_exprs\n",
    "end\n",
    "\n",
    "filename = \"thermal_no_commit.jl\"\n",
    "filepath = filepaths[filename]\n",
    "lines = minify_file(filepath)\n",
    "file_exprs = parse_lines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Any[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse_lines(Vector{String}())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{String}:\n",
       " \"\\nfunction thermal_no_commit(EP\" ⋯ 1412 bytes ⋯ \"Cap][y]\\nend)\\nend\\nreturn EP\\nend\"\n",
       " \"\\nfunction thermal_no_commit_re\" ⋯ 2206 bytes ⋯ \"Cap][y]\\nend)\\nend\\nreturn EP\\nend\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"thermal_no_commit.jl\"\n",
    "filepath = filepaths[filename]\n",
    "output_lines = Vector{String}()\n",
    "out2 = Vector{String}()\n",
    "open(filepath, \"r\") do io\n",
    "    active_str = \"\"\n",
    "    long_comment = false\n",
    "    for line in eachline(io)\n",
    "        # Check for doc strings and ignore them\n",
    "        if contains(line, \"\"\"\\\"\\\"\\\"\"\"\")\n",
    "            long_comment = !long_comment\n",
    "            continue\n",
    "        end\n",
    "        if !long_comment\n",
    "            # Remove comments\n",
    "            if !startswith(strip(line), \"#\") && line != \"\"\n",
    "                # Remove end-of-line commments\n",
    "                line = split(line, \"#\")[1]\n",
    "                # Remove tabs\n",
    "                line = replace(line, \"\\t\" => \"\")\n",
    "                active_str = string(active_str, \"\\n\", line)\n",
    "                push!(out2,active_str)\n",
    "                try Meta.parse(active_str)\n",
    "                    parse = Meta.parse(active_str)\n",
    "                    if parse.head == :incomplete\n",
    "                        continue\n",
    "                    end\n",
    "                    # if parse.head == :incomplete && !startswith(strip(active_str), \"function\")\n",
    "                        # continue\n",
    "                    # end\n",
    "                    push!(output_lines, active_str)\n",
    "                    active_str = \"\"\n",
    "                catch\n",
    "                    continue\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "output_lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
